# Lecture 1

## Motivation and Fundamentals

What does the computer see?
- Vectors


What do i see?

## Computer Vision 

Field of Computer Science that aims to make sense at image/video inputs, **identify**, **understand** and **extract relevant information**

## Scene Understandig

Aspect of Computer Vision that aims to identify and analyse objects and their context (surrounding scene, relations to other objects)

Image Segmentation

https://segment-anything.com/

Between 2D and 3D

**Rendering** (a Graphics problem)
- Given a 3D model of the scene (3D mesh, materials, lighting), and a camara, obtain an image

**Inverse Rendering** (a Vision problem)
- Given an image of a scene, infer the 3D model
- Under-constrained, ill-posed (proyective ambigueity)

- 2D -> Algorithm -> 3D

## Computer Vision: Pinhole camera model

- f - focal length
- z - depth

$\frac{v}{f} = \frac{y}{z}, \frac{u}{f} = \frac{x}{z}$
$$
Z
\begin{bmatrix}
u\\
v\\
l
\end{bmatrix}
=
\begin{bmatrix}
    f_x & 0   & c_x \\
    0   & f_y & c_y  \\
    0   & 0   & 1
\end{bmatrix}
\begin{bmatrix}
    X \\ 
    Y \\
    Z
\end{bmatrix}
$$

## Classic understanding of images

| Image | -> Feature detection -> Feature description -> Tasks:
- Matching
- Detection
- Homography estimation 

## Edge detection

- Use derivatives (in x and y direction) to obtain pixels with high gradient
- Need smoothing to reduce noise prior to taking derivative

## Corner Detetcion

- **Repeatability**
- **Saliency**
- **Locality**

Harris Corner detector algorithm
Explore intensity changes within a window as the window changes location

```python
import numpy as np
import cv2 as cv
filename = 'chessboard.png'
img = cv.imread(filename)
gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)
gray = np.float32(gray)
dst = cv.cornerHarris(gray,2,3,0.04)
#result is dilated for marking the corners, not important
dst = cv.dilate(dst,None)
# Threshold for an optimal value, it may vary depending on the image.
img[dst>0.01*dst.max()]=[0,0,255]
cv.imshow('dst',img)
if cv.waitKey(0) & 0xff == 27:
cv.destroyAllWindows()
```

## Difference of Gaussians (DoG)

Obtain Image in different scales and different amount of Gaussian blur
Keypoints obtained by computing difference of Gaussians in each scale
Choose best scale to represent that keypoint - the scale that contains a spatial gradient maxima

## Feature Descriptors

Why do we need them? To match relevant/corresponding image parts
based on their feature similarity
Properties
1. Information that is invariant w.r.t: illumination, pose, scale,
intraclass variability
2. Highly distinctive: allows for finding the correct match with a good
probability
32


## SIFT Descriptor

- Based on DoG keypoints
- Location and characteristic scale s given by DoG detector (scale invariant

## Other feature descriptors

-
-
-
-

## Image Classification performance

# Deep Learning

A type of machine learning based on artificial neural networks in which
multiple layers of processing are used to extract progressively higher
level features from data

## Supervised Learning

- Training set of _N_ samples $(x^{(i)}, y^{(i)})$
- Generated by unknown function _f_ s.t. $f(x^{(i)}) =  y^{(i)} \forall i$
- $x^{(i)}$: input, $y^{(i)}$: expected outcome
- Discover/Learn _f*_ that approximates _f_
- Given a **new** $x^{(j)}$ **with unknown** $y^{(j)}$ compute $x^{(j)}$ as $x^{(j)} = f^*(x^{(i)})$

## Training – Validation – Testing

- Use only a subset of the samples for learning _f_ (**training set**)
- The rest is for testing the quality of the predicted _f*_ (**test set**)
- If the learning process has parameters: split the training set again and tune the parameters on left-out subset (validation set)

## Artificial Neural networks


- "_...a computing system made up of a number of simple, highly
interconnected processing elements, which process information by
their dynamic state response to external inputs._"

Dr. Robert Hecht-Nielsen in "Neural Network Primer: Part I" by Maureen Caudill, Feb. 1989


### The Perceptron

- “_the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence._”

$$ f: \R^d \rightarrow \{1,0\}$$
$$ f(x)= \left\{ \begin{array}{lcc} 1 & if & w*x+b > 0 \\
0 & & otherwise \\
\end{array} \right\}
$$
$ x,w \in \R^d$ $b \in \R$

- Linear Classifier
- Only works well on linearly separable problem

![Perceptron](/imgs/perceptron1.png)

## More layers

## Activation function

Layers are typically followed by **non-linear** activation functions, that act per neuron

examples:

- **Sigmoid**: $\sigma(x) = \frac{1}{1+e^{-x}}$

- **Tanh**: $tanh(x)= 2\sigma(2x)-1$

- **ReLU (Rectified Lienar Unit)**: $f(x)=\max(0,x)$

    - simplfy thresholds at zero
    - Sparse activation
    - Computationally efficient
    - Non-saturating -> speeds up convergence

## How to train your network

- Set of _N_ samples $$
- Define loss $\mathcal{L}(f(x),x) \in \R$ to measure error for a sample
- E.g. for regression task: _mean squared error_ (MSE) or _mean absolute error_ (MAE) are common.
- **Training**: find weights that minimize $\sum_i \mathcal{L}(f(x)^{(i)},y^{(i)})$ for the samples
- Often uses simple gardient descent methods
- **Backpropagation** computes the gradients of all parameters

## Gradient Descent

How to use **derivatives**?

- **Gradient descent** to minimize error function
    $$ w^{(t+1)} = w^{(t)} - \lambda \frac{\partial \mathcal{L}}{\partial w} $$
    - Update the weights in every iteration of training with a small gradient step
    - Learning rate $\lambda$ adjust the step size
    - Error is defined over the **whole** training set: $\sum \mathcal{L}(x^{(i)})$
    - Need to compute and sum the derivatives of all samples before one gradient step
    - Slow but accurate updates

- **Stochastic gradient descent (SGD)**
    
    - Approximate derivative from small, random subset (mini batch) of training set
    - Noisy but faster
    - Usually: make sure to see every sample the same amount of times (epochs)

## Backpropagation

- Backporpagation is an efficient way to compute **derivatives**
- Using (stored) **activations** from the foward pass
- Backpropagating information from layer $i + 1$ to $i$ and reusing already know (previously computed) derivatives
- Possible through chain rule

# Pytorch

## Overview

- **Open source** machine learning library based on the Torch library
- Operates on multi-dimensional vectors (**Tensors**)
- Can execute on the CPU, GPU, distributed systems, etc.
- **Dynamic** computational graph (can change on runtime)
    - Nodes: Tensors
    - Edges: mathematical operations
- Performs **automatic differentiation**
- Python and C++ interface
- **Torchvision**: package that implements many important vision algorithms 

_torch.nn.Module_

- Base class for a neural network module
- Can contain sub-modules
- Inherit this class when creating your neural network

_torch.nn.Parameter_

- Learnable tensor

_torch.nn.functional_

- A set of operations such as convolution, activation, etc