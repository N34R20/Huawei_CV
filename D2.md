# Lecture 2

## From ANNs to CNNs

Some issues with ANNs:

- They don't scale with high-dimensional inputs, increasing number of neurons, layers, etc

- They do not take advantage of the input structure and ignore the local arrangement of neurons. For structured inputs (e.g. images), this is a disadvantage

- Similar weight patterns would need to be learned for different regions, 

## Towards CNNs

COnvolutional Neural Networks (CNNs) are a category of multi-layer NNs with learable weights and biases, designed such that they tackle commun problems of ANNs

- **Asumption**:
    Inputs are structured, e.g. images

- **Key Idea**:
    **Inavriance** to shifts, scale and small distortions using
    - local
    -
    -

- **Main operations**:


## CNN Architecture

**Network architecture** generally composed by:
-
-
-

## LeNet - 5
-
-
-

# Basic Layers 

## **Input Layer**

-
-
    -
    -   


## **Convolutional Layer**

- Convolution in PyTorch

```python
torch.nn.Conv2d(
    in_channels,
    out_channels,
    kernel_size,
    stride=1,
    padding=0,
    dilation=1,
    groups=1,
    bias=True,
    padding_mode='zeros'

)
```


## **Activation function**

Convolutions are typically followed by **non-linear** activation functions, that act per neuron.

Rectified Linear Unit (ReLU)

"Dying ReLU": neurons might be driven into an irreversible state due to specific weight updates () and thus stay inactive 

## **Pooling Layer**

Performs an element-wise operation on the feature maps, on each channel independently, Usually **max()** or **avg()**

- (Max)Pooling in PyTorch

```python
torch.nn.MaxPool2d(
    kernel_size,
    stride=None,
    padding=0,
    dilation=1,
    return_indices=False,


)
```

## **Fully-connected Layer**

-
-
-
