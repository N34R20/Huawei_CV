# Lecture 2

## From ANNs to CNNs

Some issues with ANNs:

- They don't scale with high-dimensional inputs, increasing number of neurons, layers, etc

- They do not take advantage of the input structure and ignore the local arrangement of neurons. For structured inputs (e.g. images), this is a disadvantage

- Similar weight patterns would need to be learned for different regions, where similar features repeat. Need for large datasets that cover all variations.

## Towards CNNs

COnvolutional Neural Networks (CNNs) are a category of multi-layer NNs with learable weights and biases, designed such that they tackle commun problems of ANNs

- **Asumption**:
    Inputs are structured, e.g. images

- **Key Idea**:
    **Inavriance** to shifts, scale and small distortions using
    - local weighted connections, i.e. local **receptive field**
    - **shared weights** across spatial locations
    - spatial **sub-sampling**

- **Main operations**:
    - Dot products with weights ($W^T x$) and addition of biases $b$
    - Non-linearities
    - max() functions


## CNN Architecture

**Network architecture** generally composed by:
-
-
-

## LeNet - 5
-
-
-

# Basic Layers 

## **Input Layer**

-
-
    -
    -   


## **Convolutional Layer**

- Convolution in PyTorch

```python
torch.nn.Conv2d(
    in_channels,
    out_channels,
    kernel_size,
    stride=1,
    padding=0,
    dilation=1,
    groups=1,
    bias=True,
    padding_mode='zeros'

)
```


## **Activation function**

Convolutions are typically followed by **non-linear** activation functions, that act per neuron.

Rectified Linear Unit (ReLU)

"Dying ReLU": neurons might be driven into an irreversible state due to specific weight updates () and thus stay inactive 

## **Pooling Layer**

Performs an element-wise operation on the feature maps, on each channel independently, Usually **max()** or **avg()**

- (Max)Pooling in PyTorch

```python
torch.nn.MaxPool2d(
    kernel_size,
    stride=None,
    padding=0,
    dilation=1,
    return_indices=False,


)
```

## **Fully-connected Layer**

-
-
-

# Pytorch

## Define a Neural Network Model

- Extend the Module class of torch.nn
- Implement the constructor and the forward member function

```python
import torch.nn as nn
class ConvNet(nn.Module):
def __init__(self):
super(ConvNet, self).__init__()
self.pool = nn.AvgPool2d(2, 2)
self.conv = nn.Conv2d(3, 16, 3)
self.fc = nn.Linear(16 * 5 * 5, 60)
self.relu = nn.Relu()
def forward(self, x):
x = self.pool(self.relu(self.conv(x)))
x = x.view(-1, 16 * 5 * 5)
x = self.fc(x)
return x

```

- Alternatively, use _torch.nn.Sequential_


```python
import torch.nn as nn
class ConvNet(nn.Module):
def __init__(self):
super(ConvNet, self).__init__()
self.backbone = nn.Sequential(nn.Conv2d(3, 16, 3),
nn.Relu(),
nn.AvgPool2d(2, 2))
self.fc = nn.Linear(16 * 5 * 5, 60)
def forward(self, x):
x = self.backbone(x)
x = x.view(-1, 16 * 5 * 5)
x = self.fc(x)
return x

```

## Layer Visualization 

## Weight Initialization

## Loss function

## Regularization

## Optimization Methods


## Hyperparameter tuning

## How to train your network

## TensorBoard: Visualizing Learning

## Saving / Restoring weights


# CNNs for Computer Vision Tasks